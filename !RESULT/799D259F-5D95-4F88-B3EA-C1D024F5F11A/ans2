Алгоритм обратного распространения ошибки является одним из методов обучения многослойных нейронных сетей прямого распространения, называемых также многослойными персептронами. Многослойные персептроны успешно применяются для решения многих сложных задач.
Обучение алгоритмом обратного распространения ошибки предполагает два прохода по всем слоям сети: прямого и обратного. При прямом проходе входной вектор подается на входной слой нейронной сети, после чего распространяется по сети от слоя к слою. В результате генерируется набор выходных сигналов, который и является фактической реакцией сети на данный входной образ. Во время прямого прохода все синаптические веса сети фиксированы. Во время обратного прохода все синаптические веса настраиваются в соответствии с правилом коррекции ошибок, а именно: фактический выход сети вычитается из желаемого, в результате чего формируется сигнал ошибки. Этот сигнал впоследствии распространяется по сети в направлении, обратном направлению синаптических связей. Отсюда и название – алгоритм обратного распространения ошибки. 
Алгоритм включает следующую последовательность действий:
1. Инициализировать синаптические веса маленькими случайными значениями.
2. Выбрать очередную обучающую пару из обучающего множества; подать входной вектор на вход сети.
3. Вычислить выход сети.
4. Вычислить разность между выходом сети и требуемым выходом (целевым вектором обучающей пары).
5. Подкорректировать веса сети для минимизации ошибки.
6. Повторять шаги с 2 по 5 для каждого вектора обучающего множества до тех пор, пока ошибка на всем множестве не достигнет приемлемого уровня.
Операции, выполняемые шагами 2 и 3, сходны с теми, которые выполняются при функционировании уже обученной сети, т.е. подается входной вектор и вычисляется получающийся выход. Вычисления выполняются послойно. Шаги 4 и 5 составляют "обратный проход", здесь вычисляемый сигнал ошибки распространяется обратно по сети и используется для подстройки весов.
Перед обучением необходимо разбить имеющиеся пары "вход – выход" на две части: обучающие и тестовые.
Тестовые пары используются для проверки качества обучения – НС хорошо обучена, если выдает при заданной тестовой парой входе выход, близкий к тестовому.
При обучении возможна ситуация, когда НС показывает хорошие результаты для обучающих данных, но плохие – для тестовых данных. Здесь могут быть две причины:
1. Тестовые данные сильно отличаются от обучающих, т.е. обучающие пары охватывали не все области входного пространства.
2. Возникло явление "переобучения" (overfitting) НС, когда поведение НС оказывается более сложным, чем решаемая задача.
Переобучение НС — явление, когда построенная модель хорошо объясняет примеры из обучающей выборки, но относительно плохо работает на примерах, не участвовавших в обучении.
Если в результате обучения нейронная сеть хорошо распознает примеры из обучающего множества, но не приобретает свойство обобщения, т.е. не распознает или плохо распознает любые другие примеры, кроме обучающих, то говорят, что сеть переобучена. Переобучение — это результат чрезмерной подгонки НС к обучающим примерам.
