Нейронные сети GRNN (Generalized Regression Neural Network) предназначены для решения задач обобщенной регрессии, анализа временных рядов и аппроксимации функций. Характерной особенностью этих сестей является очень высокая скорость их обучения.
Они представляют собой комбинацию из двух сетей RBF (1 скрытый слой) и MLP (2 скрытый слой), что и определило встречные направления процессов обучения нейронов сети. Как и для RBF число слоев первого слоя примерно соответствует мощности обучающего множества (количеству возможных кластеров-шаблонов, описывающих входные данные), для второго же промежуточного слоя (MLP) число нейронов на слое MLP на единицу больше, чем мощность вектора Y.
Наиболее начимые преимущества GRNN-сетей состоят в том, что выходное значение имеет вероятностный смысл (и поэтому его легче интерпретировать).
Архитектура сети PNN базируется на архитектуре радиальной базисной сети, но в качестве второго слоя использует так называемый конкурирующий слой, который подсчитывает вероятность принадлежности входного вектора к тому или иному классу и в конечном счете сопостовляет вектор с тем классом, вероятность принадлежности к которому выше.
Сети PNN могут весьма эффективно применяться для решения задач классификации. если задано достаточно большое обучающее множество, то решения, генерируемые сетями, сходятся к решениям, соответствующим правилу Бейеса. Недостаток сетей GRNN и PNN заключается в том, что работают они относительно медленно, поскольку выполняют очень большие объемы вычислений по сравнению с другими типами нейронных сетей.
